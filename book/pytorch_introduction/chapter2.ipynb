{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch深度学习框架简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu113\n"
     ]
    }
   ],
   "source": [
    "# 版本\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch支持的数据类型\n",
    "print(torch.float is torch.float32)\n",
    "print(torch.double is torch.float64)\n",
    "print(torch.half is torch.float16)\n",
    "print(torch.short is torch.int16)\n",
    "print(torch.int is torch.int32)\n",
    "print(torch.long is torch.int64)\n",
    "# 另外还支持\n",
    "# torch.uint8 torch.int8 torch.bool\n",
    "print(\"=\"*9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "torch.int64\n",
      "torch.LongTensor\n",
      "=========\n",
      "torch.float32\n",
      "torch.float64\n",
      "=========\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64)\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)\n",
      "=========\n",
      "tensor([[0.1355, 0.1292, 0.1756],\n",
      "        [0.6181, 0.2374, 0.5208],\n",
      "        [0.9602, 0.5415, 0.4768]])\n",
      "tensor([[[-1.0044, -0.5003,  0.4817,  1.0805,  0.2029],\n",
      "         [ 1.6441, -0.2607, -1.0867,  1.1477,  0.3658],\n",
      "         [ 1.4103, -0.6263, -0.5787, -1.0953, -1.6389],\n",
      "         [-1.2681,  0.2339, -0.4071,  0.4023, -1.2800]],\n",
      "\n",
      "        [[-1.0553,  0.9620,  0.0335, -0.7045, -0.0882],\n",
      "         [-2.0618, -3.7923,  3.2679, -1.7934,  1.1875],\n",
      "         [-0.5285, -1.0066,  1.8365,  0.1928,  0.9412],\n",
      "         [-1.3469,  1.6225, -0.8181, -1.4446,  0.7415]],\n",
      "\n",
      "        [[ 0.5228, -0.4694,  0.8319,  0.9243,  0.6221],\n",
      "         [-0.7876, -1.7585, -1.3794, -0.4983, -1.4728],\n",
      "         [ 0.2537,  1.5355, -0.6539,  0.4416,  0.0738],\n",
      "         [-0.7016,  0.7648, -0.0900,  0.0943, -0.7359]]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([9, 2, 8, 5, 4, 4, 7, 9, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import tensor\n",
    "# print(torch.tensor([1,2,3,4]))\n",
    "print(torch.tensor([1,2,3,4]))\n",
    "print(torch.tensor([1,2,3,4]).dtype)\n",
    "print(torch.tensor([1,2,3,4]).type())\n",
    "print(\"=\"*9)\n",
    "\n",
    "# 从numpy创建\n",
    "# torch的默认浮点是32位的，numpy是64位的\n",
    "print(torch.tensor([1.0, 2.0, 3.0, 4.0]).dtype)\n",
    "print(torch.tensor(np.array([1.0, 2.0, 3.0, 4.0])).dtype)\n",
    "print(\"=\"*9)\n",
    "\n",
    "# 类型转换\n",
    "float_tensor = torch.tensor(np.arange(10), dtype=torch.float64)\n",
    "print(float_tensor)\n",
    "int_tensor = float_tensor.to(torch.int)\n",
    "print(int_tensor)\n",
    "print(\"=\"*9)\n",
    "\n",
    "# 通过内置函数创建\n",
    "print(torch.rand(3, 3)) # 均匀分布\n",
    "print(torch.randn(3, 4, 5)) # 标准正态分布\n",
    "print(torch.zeros(2, 3)) # 全0矩阵\n",
    "print(torch.ones(2, 3)) # 全1矩阵\n",
    "print(torch.eye(4)) # 单位矩阵\n",
    "print(torch.randint(1, 10, (10, ))) # 随机整数\n",
    "\n",
    "# 使用like系创建\n",
    "base = torch.randn(3,4)\n",
    "one_like = torch.ones_like(base)\n",
    "zero_like = torch.zeros_like(base)\n",
    "random_like = torch.rand_like(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的存储设备\n",
    "只有都在cpu，或都在gpu的同一张卡上，张量之间才能互相运算，否则会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:5\n",
      "cpu\n",
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(np.linspace(-1, 1, 100), dtype=torch.float, device=\"cpu\")\n",
    "print(tensor.device)\n",
    "tensor = tensor.cuda(5)\n",
    "print(tensor.device)\n",
    "tensor = tensor.cpu()\n",
    "print(tensor.device)\n",
    "tensor = tensor.to(\"cuda:5\")\n",
    "print(tensor.device)\n",
    "tensor = tensor.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量维度相关的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "24\n",
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4]) 4\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "93876346295232\n",
      "93876346295232\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 0, 1],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(2, 3, 4)\n",
    "# 维度数量\n",
    "print(t.ndim, t.ndimension())\n",
    "# 元素总数\n",
    "print(t.nelement())\n",
    "# size与shape\n",
    "print(t.size(), t.shape, t.size(-1))\n",
    "\n",
    "# 重整shape, 可以使用.reshape方法，也可以使用view方法。\n",
    "# reshape方法可以确保合法的重整必定成功，但view只有在满足兼容条件时才能成功？\n",
    "# view必定不做拷贝，reshape也许会拷贝。\n",
    "new_tensor = torch.ones(4, 4, dtype=torch.int)\n",
    "another_tensor = new_tensor.reshape(2, 8)\n",
    "print(another_tensor)\n",
    "print(new_tensor.data_ptr(), another_tensor.data_ptr(), sep=\"\\n\")\n",
    "new_tensor[2, 2]=0\n",
    "print(new_tensor)\n",
    "print(another_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量切片与方便的掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7314,  0.3722, -0.4829, -1.3744],\n",
      "         [ 1.0166,  1.1601, -0.9600,  0.2867],\n",
      "         [ 0.4411,  0.9263,  0.7633,  1.0894]],\n",
      "\n",
      "        [[ 1.8161, -0.5918, -1.3780, -0.8696],\n",
      "         [-1.9779, -0.7498, -1.8901,  0.6590],\n",
      "         [-1.2350, -1.2828, -2.5658, -0.9084]]])\n",
      "tensor(-0.9084) tensor(-0.9084)\n",
      "tensor([ 1.0894, -0.9084])\n",
      "tensor([[[0, 1, 0, 0],\n",
      "         [1, 1, 0, 1],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 0, 0, 1],\n",
      "         [0, 0, 0, 0]]], dtype=torch.uint8)\n",
      "tensor([0.3722, 1.0166, 1.1601, 0.2867, 0.4411, 0.9263, 0.7633, 1.0894, 1.8161,\n",
      "        0.6590])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(2, 3, 4)\n",
    "print(t)\n",
    "print(t[1, 2, 3], t[1][2][3])\n",
    "print(t[:, 2, 3])\n",
    "# 方便的掩码\n",
    "print((t>0).to(torch.uint8))\n",
    "print(t[t>0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量自运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1074, 0.0795, 0.2458, 0.9804],\n",
      "        [0.6620, 0.0442, 0.5432, 0.4888],\n",
      "        [0.9265, 0.6012, 0.3177, 0.2526]])\n",
      "tensor([[0.3278, 0.2820, 0.4958, 0.9902],\n",
      "        [0.8136, 0.2102, 0.7370, 0.6991],\n",
      "        [0.9626, 0.7754, 0.5636, 0.5026]])\n",
      "tensor(0.4374)\n",
      "tensor(5.2493)\n",
      "tensor([2, 2, 1, 0])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.9265, 0.6012, 0.5432, 0.9804]),\n",
      "indices=tensor([2, 2, 1, 0]))\n",
      "=========\n",
      "tensor([[0.1074, 0.0795, 0.2458, 0.9804],\n",
      "        [0.6620, 0.0442, 0.5432, 0.4888],\n",
      "        [0.9265, 0.6012, 0.3177, 0.2526]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.rand(3,4)\n",
    "print(t)\n",
    "print(t.sqrt())\n",
    "print(t.mean()) # 通过指定维度，可以求取特定维度的mean与sum等\n",
    "print(t.sum())\n",
    "print(t.argmax(0))\n",
    "print(t.max(0))\n",
    "print(\"=\"*9)\n",
    "# 拷贝向量\n",
    "nt = torch.zeros_like(t)\n",
    "# 在torch中，带有_的方法是原地更改的，故不需要写nt=...\n",
    "nt.copy_(t)\n",
    "print(nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量间运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([-9, -7, -5, -3, -1,  1,  3,  5,  7]) tensor([10, 18, 24, 28, 30, 30, 28, 24, 18]) tensor([0.1000, 0.2222, 0.3750, 0.5714, 0.8333, 1.2000, 1.7500, 2.6667, 4.5000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.tensor(range(1, 10))\n",
    "t2 = torch.tensor(range(10, 1, -1))\n",
    "\n",
    "print(t1+t2)\n",
    "print(t1.add(t2))\n",
    "print(torch.add(t1, t2))\n",
    "print(t1-t2, t1*t2, t1/t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵乘法，批乘法与爱因斯坦求和约定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5754, -1.0357, -1.2547,  0.0307],\n",
      "        [-0.7101, -0.6728,  0.8748,  2.8021]]) tensor([[-0.5754, -1.0357, -1.2547,  0.0307],\n",
      "        [-0.7101, -0.6728,  0.8748,  2.8021]])\n",
      "=========\n",
      "tensor([[[-0.9862,  2.0148, -0.0999, -0.1824],\n",
      "         [-3.8542,  0.4459, -4.2813, -0.7014]],\n",
      "\n",
      "        [[-1.3157, -0.8132, -1.7206, -2.4205],\n",
      "         [ 0.0772, -1.0718,  1.1261,  0.9608]],\n",
      "\n",
      "        [[-0.2986,  0.2007,  3.0160,  0.6551],\n",
      "         [ 1.3310,  0.1772, -0.0059, -1.0315]]]) tensor([[[-0.9862,  2.0148, -0.0999, -0.1824],\n",
      "         [-3.8542,  0.4459, -4.2813, -0.7014]],\n",
      "\n",
      "        [[-1.3157, -0.8132, -1.7206, -2.4205],\n",
      "         [ 0.0772, -1.0718,  1.1261,  0.9608]],\n",
      "\n",
      "        [[-0.2986,  0.2007,  3.0160,  0.6551],\n",
      "         [ 1.3310,  0.1772, -0.0059, -1.0315]]])\n",
      "tensor([[[-0.9862,  2.0148, -0.0999, -0.1824],\n",
      "         [-3.8542,  0.4459, -4.2813, -0.7014]],\n",
      "\n",
      "        [[-1.3157, -0.8132, -1.7206, -2.4205],\n",
      "         [ 0.0772, -1.0718,  1.1261,  0.9608]],\n",
      "\n",
      "        [[-0.2986,  0.2007,  3.0160,  0.6551],\n",
      "         [ 1.3310,  0.1772, -0.0059, -1.0315]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 矩阵乘法\n",
    "t1 = torch.randn(2, 3)\n",
    "t2 = torch.randn(3, 4)\n",
    "print(torch.mm(t1, t2), t1@t2)\n",
    "\n",
    "print(\"=\"*9)\n",
    "# 批乘法\n",
    "t1 = torch.randn(3, 2, 3)\n",
    "t2 = torch.randn(3, 3, 4)\n",
    "# 两种写法是相同的，torch可以自己判断出是批乘法\n",
    "print(torch.bmm(t1, t2), t1@t2)\n",
    "\n",
    "# 爱因斯坦求和约定\n",
    "print(torch.einsum(\"abc,acd->abd\", t1, t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量拼接与分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16])\n",
      "torch.Size([4, 3, 4])\n",
      "(tensor([[-1.4122],\n",
      "        [ 0.9945],\n",
      "        [ 0.0597]]), tensor([[-3.5429, -0.2772],\n",
      "        [-1.0728,  0.8370],\n",
      "        [-1.0781, -0.2682]]), tensor([[ 0.5112],\n",
      "        [-1.5331],\n",
      "        [ 0.2329]]))\n",
      "(tensor([[-1.4122, -3.5429],\n",
      "        [ 0.9945, -1.0728],\n",
      "        [ 0.0597, -1.0781]]), tensor([[-0.2772,  0.5112],\n",
      "        [ 0.8370, -1.5331],\n",
      "        [-0.2682,  0.2329]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.randn(3, 4)\n",
    "t2 = torch.randn(3, 4)\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(3, 4)\n",
    "# cat不会新增维度\n",
    "print(torch.cat([t1, t2, t3, t4], -1).shape) # 这样，第一个维度是相同的，可以拼接，第二个维度会合并,得(3, 16)\n",
    "# stack是新增维度的，dim决定了新增的维度放置在第几维\n",
    "print(torch.stack([t1, t2, t3, t4], dim=0).shape)\n",
    "# split告诉torch怎么分,这里将4维度分成1，2，1\n",
    "print(t1.split([1, 2, 1], dim=-1))\n",
    "# chunk告诉torch分成几个, 这里，把最后一个维度均分成2个。\n",
    "print(t1.chunk(2, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 消除与增加1维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 增加为1的维度对任何张量都是可以的\n",
    "t = torch.randn(3, 4)\n",
    "print(t.shape)\n",
    "t.unsqueeze_(1)\n",
    "print(t.shape)\n",
    "\n",
    "# 可以方便地消除为1的维度\n",
    "print(t.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加与消除维度对张量的广播是必要的。使得张量能与另一个张量之间达成维度的匹配，以便广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 5])\n",
      "tensor([[[ 0.1089, -1.4662,  0.2281, -2.1201, -0.3451],\n",
      "         [ 1.9122, -0.8835,  1.6466, -0.7086, -1.4303],\n",
      "         [-0.0524,  0.1557, -0.3267, -0.6480, -1.1932],\n",
      "         [-0.7810, -1.5105,  0.4413, -0.3786, -2.4496]],\n",
      "\n",
      "        [[-0.4762,  2.1753, -3.6313, -0.4530,  1.4494],\n",
      "         [ 0.7417,  0.8084, -2.0541, -1.7982, -1.1202],\n",
      "         [ 1.4720,  1.9049, -1.7990, -2.3076,  1.0320],\n",
      "         [-1.3049,  1.5306, -3.9112, -1.3204,  0.6329]],\n",
      "\n",
      "        [[ 1.5535, -0.9278,  1.1954, -0.4431,  1.3355],\n",
      "         [-0.4347, -1.4892,  0.5054,  0.0472,  1.5042],\n",
      "         [-0.5629, -1.1739,  1.3661,  0.8342,  3.1323],\n",
      "         [ 0.7827, -0.4685,  0.3974, -0.2244,  2.0698]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.randn(3, 4, 5)\n",
    "t2 = torch.randn(3, 5)\n",
    "# 要让t1+t2，直接这样做是不行的，故对t2升维\n",
    "t2.unsqueeze_(1)\n",
    "print(t2.shape)\n",
    "# 这样，在进行t1+t2时，第一个维度会进行广播。\n",
    "print(t1+t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('espnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c085eeb1cb103ec2f9784dc08c37e4d20172fae9f66eaf20347152a3e5e0aa2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
