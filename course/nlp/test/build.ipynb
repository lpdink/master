{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpdink/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是', '一', '段', '测试', '数据', '。']\n",
      "['This', 'is', 'a', 'piece', 'of', 'test', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "zh_tokenizer = get_tokenizer(\"spacy\", \"zh_core_web_sm\")\n",
    "en_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "print(zh_tokenizer(\"这是一段测试数据。\"))\n",
    "print(en_tokenizer(\"This is a piece of test data.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data && DataLoader\n",
    "\n",
    "\n",
    "class CorpusData:\n",
    "    \"\"\"\n",
    "    单一语种，训练数据不大，直接读到内存里\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, tokenizer) -> None:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "        # 构建词表\n",
    "        symbols = [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]\n",
    "        word2id = {word: id for id, word in enumerate(symbols)}\n",
    "        id2word = {id: word for id, word in enumerate(symbols)}\n",
    "        split_sents = []\n",
    "        max_words = 0\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            words = tokenizer(line)\n",
    "            max_words = max(max_words, len(words))\n",
    "            split_sents.append(words)\n",
    "            for word in words:\n",
    "                if word not in word2id.keys():\n",
    "                    id = max(id2word.keys()) + 1\n",
    "                    word2id[word] = id\n",
    "                    id2word[id] = word\n",
    "        self._word2id = word2id\n",
    "        self._id2word = id2word\n",
    "        self._split_sents = split_sents\n",
    "        self._vocab = None\n",
    "        self.max_words = max_words\n",
    "\n",
    "    def get_word(self, id):\n",
    "        return self._id2word.get(id, None)\n",
    "\n",
    "    def get_id(self, word):\n",
    "        return self._word2id.get(word, None)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        if self._vocab is None:\n",
    "            self._vocab = sorted(\n",
    "                list(self._word2id.keys()), key=lambda word: self._word2id[word]\n",
    "            )\n",
    "        return self._vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._split_sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._split_sents[index]\n",
    "\n",
    "\n",
    "class TranslationData:\n",
    "    def __init__(self, src_data_obj, dst_data_obj) -> None:\n",
    "        self.src_data_obj = src_data_obj\n",
    "        self.dst_data_obj = dst_data_obj\n",
    "        assert len(src_data_obj) == len(\n",
    "            dst_data_obj\n",
    "        ), f\"src_data_obj len {len(src_data_obj)} not eq to dst_data_obj len {len(dst_data_obj)}\"\n",
    "        self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        将sents转id\n",
    "        \"\"\"\n",
    "        train_data, test_data, valid_data = [], [], []\n",
    "        sent_num = len(self.src_data_obj)\n",
    "        index = 0\n",
    "        for src_words, dst_words in zip(self.src_data_obj, self.dst_data_obj):\n",
    "            src_tensor = torch.tensor(\n",
    "                [self.src_data_obj.get_id(\"<bos>\")]\n",
    "                + [self.src_data_obj.get_id(word) for word in src_words]\n",
    "                + [self.src_data_obj.get_id(\"<eos>\")]\n",
    "                + [self.src_data_obj.get_id(\"<pad>\")]\n",
    "                * (self.src_data_obj.max_words - len(src_words))\n",
    "            )\n",
    "            dst_tensor = torch.tensor(\n",
    "                [self.dst_data_obj.get_id(\"<bos>\")]\n",
    "                + [self.dst_data_obj.get_id(word) for word in dst_words]\n",
    "                + [self.dst_data_obj.get_id(\"<eos>\")]\n",
    "                + [self.dst_data_obj.get_id(\"<pad>\")]\n",
    "                * (self.dst_data_obj.max_words - len(dst_words))\n",
    "            )\n",
    "            if (index / sent_num) < config.train_pert:\n",
    "                train_data.append((src_tensor, dst_tensor))\n",
    "            elif (index / sent_num) < config.train_pert + config.test_pert:\n",
    "                test_data.append((src_tensor, dst_tensor))\n",
    "            else:\n",
    "                valid_data.append((src_tensor, dst_tensor))\n",
    "            index += 1\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.valid_data = valid_data\n",
    "\n",
    "    def get_data(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_data, batch_size=config.batch_size, shuffle=True\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.generate_batch,\n",
    "        )\n",
    "        valid_iter = DataLoader(\n",
    "            self.valid_data, batch_size=config.batch_size, shuffle=True\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3ef322759e63a9850931fefd995ee0c50eb2ab97c80f148d007280f93220e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
